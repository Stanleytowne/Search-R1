"""
ToolBenchæ¨¡å¼çš„Rewardè®¡ç®—
åŒ…æ‹¬ï¼š
1. æ ¼å¼å¥–åŠ±ï¼šå¥–åŠ±æ¨¡å‹ç”Ÿæˆæ­£ç¡®çš„æ ¼å¼ï¼ˆThought/Action/Action Inputï¼‰
2. Function callæ­£ç¡®å¥–åŠ±ï¼šå¦‚æœAPIè°ƒç”¨ç»“æœæœ‰errorï¼Œåˆ™æƒ©ç½š
3. Finishè°ƒç”¨å¥–åŠ±ï¼šæœ€åä¸€æ¬¡æ˜¯å¦è°ƒç”¨Finish
"""

import torch
import re
import json
from typing import List, Dict
import requests
from verl import DataProto


class ToolBenchRewardManager:
    """ToolBenchæ¨¡å¼çš„Rewardç®¡ç†å™¨"""
    
    def __init__(
        self,
        tokenizer,
        num_examine: int = 0,
        reward_server_url: str = "http://localhost:8000/evaluate_batch"
    ):
        """
        Args:
            tokenizer: Tokenizerç”¨äºè§£ç 
            num_examine: æ‰“å°çš„æ ·æœ¬æ•°é‡
        """
        self.tokenizer = tokenizer
        self.num_examine = num_examine
        self.reward_server_url = reward_server_url
    
    def __call__(self, data: DataProto) -> torch.Tensor:
        """
        è®¡ç®—ToolBenchæ¨¡å¼çš„reward
        
        Args:
            data: DataProtoåŒ…å«ç”Ÿæˆçš„æ•°æ®å’Œmeta_info
            
        Returns:
            token_level_rewards: (batch_size, response_length)çš„reward tensor
        """
        # if 'rm_scores' in data.batch.keys(), return the rm_scores
        if 'rm_scores' in data.batch.keys():
            return data.batch['rm_scores']
        
        batch_size = data.batch['responses'].shape[0]
        response_length = data.batch['responses'].shape[1]
        
        # init reward tensor
        reward_tensor = torch.zeros((batch_size, response_length), dtype=torch.float32)
        
        # get ToolBench related information from meta_info
        meta_info = data.meta_info

        all_queries = []
        all_trajectories = []
        each_turn_end_loc = [[] for _ in range(batch_size)]
        for i in range(batch_size):
            data_item = data[i]

            response_ids = data_item.batch['responses']
            prompt_ids = data_item.batch['prompts']
            prompt_length = prompt_ids.shape[-1]
            attention_mask = data_item.batch['attention_mask']

            valid_prompt_length = attention_mask[:prompt_length].sum().item()
            valid_response_length = attention_mask[prompt_length:].sum().item()
            
            valid_prompt_ids = prompt_ids[-valid_prompt_length:] if valid_prompt_length > 0 else prompt_ids
            valid_response_ids = response_ids[:valid_response_length] if valid_response_length > 0 else response_ids

            prompt_str = self.tokenizer.decode(valid_prompt_ids, skip_special_tokens=False)
            query_str = self._extract_query(prompt_str)
            response_str = self.tokenizer.decode(valid_response_ids, skip_special_tokens=False)
            
            all_queries.append(query_str)
            all_trajectories.append(response_str)

            # get the end location of each turn
            full_info_mask = data_item.batch['info_mask']
            response_mask = full_info_mask[prompt_length : prompt_length + valid_response_length]
            
            mask_list = response_mask.tolist()
            turn_indices = []
            
            for t, is_model_token in enumerate(mask_list):
                if is_model_token == 1 and (t == len(mask_list) - 1 or mask_list[t + 1] == 0):
                    turn_indices.append(t)
            
            each_turn_end_loc[i] = turn_indices
            assert len(each_turn_end_loc[i]) == meta_info['turns_stats'][i], f"Sample {i} has turns stats as {meta_info['turns_stats'][i]} and each turn end loc as {each_turn_end_loc[i]}"

            if i < self.num_examine:
                print(f"\n{'='*20} [DEBUG REWARD LOC] Sample {i} {'='*20}")
                print(f"Calculated Indices: {each_turn_end_loc[i]}")
                
                # è·å–ç”¨äºæ˜¾ç¤ºçš„ token ID å’Œ mask
                # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ valid_response_idsï¼Œç¡®ä¿åªæ‰“å°æœ‰æ•ˆéƒ¨åˆ†
                debug_tokens = valid_response_ids.tolist()
                debug_mask = mask_list  # æ²¿ç”¨ä¸Šé¢è®¡ç®—å‡ºçš„ list
                
                print("\n[Visualized Response Flow]")
                print("Legend: [M] = Model Token (Mask=1), [E] = Env Token (Mask=0), ğŸ“ = Reward Location")
                print("-" * 60)
                
                # é€ä¸ª Token è¿˜åŸå¹¶æ‰“å°ï¼Œé‡åˆ°å…³é”®ä½ç½®æ¢è¡Œæˆ–æ ‡è®°
                buffer_str = ""
                current_type = debug_mask[0] if len(debug_mask) > 0 else 1
                
                for idx, (tid, is_model) in enumerate(zip(debug_tokens, debug_mask)):
                    token_str = self.tokenizer.decode([tid], skip_special_tokens=False)
                    
                    # ç®€å•å¤„ç†æ¢è¡Œç¬¦ï¼Œé˜²æ­¢æ‰“å°æ··ä¹±
                    token_str_repr = token_str.replace('\n', '\\n')
                    
                    # æ ‡è®°æ˜¯å¦æ˜¯ Reward ä½ç½®
                    is_reward_loc = idx in each_turn_end_loc[i]
                    
                    # å¦‚æœ mask ç±»å‹å‘ç”Ÿå˜åŒ–ï¼ˆä»æ¨¡å‹->ç¯å¢ƒ æˆ– ç¯å¢ƒ->æ¨¡å‹ï¼‰ï¼Œå…ˆæ‰“å°ä¹‹å‰çš„ buffer
                    if is_model != current_type:
                        prefix = "[Model]: " if current_type == 1 else "[Env]:   "
                        print(f"{prefix}{buffer_str}")
                        buffer_str = ""
                        current_type = is_model
                    
                    # æ‹¼æ¥åˆ° buffer
                    buffer_str += token_str
                    
                    # å¦‚æœè¿™é‡Œæ˜¯ Reward ä½ç½®ï¼Œæ’å…¥æ˜¾çœ¼æ ‡è®°
                    if is_reward_loc:
                        buffer_str += " [ğŸ“REWARD] "
                
                # æ‰“å°å‰©ä½™çš„ buffer
                if buffer_str:
                    prefix = "[Model]: " if current_type == 1 else "[Env]:   "
                    print(f"{prefix}{buffer_str}")
                
                print("="*60 + "\n")
        
        pass_rewards = self._get_remote_pass_rewards(all_queries, all_trajectories)
        data.meta_info['pass_rewards'] = pass_rewards

        if data[0].non_tensor_batch['data_source'] == 'toolbench-eval':
            for i in range(batch_size):
                last_turn_end_loc = each_turn_end_loc[i][-1]
                reward_tensor[i, last_turn_end_loc] = pass_rewards[i]
                
                if i < self.num_examine:
                    response_str = all_trajectories[i]
                    print(f"\n[Eval Reward Sample {i}]")
                    print(f"  Response: {response_str[:200]}...")
                    print(f"  Pass reward: {pass_rewards[i]:.3f}")
            return reward_tensor

        # 1. format and function call reward for each turn (excluding the final turn)
        format_and_function_call_reward = self._compute_format_and_function_call_reward(meta_info)
        # 2. finish reward for the final turn
        finish_reward = self._compute_finish_reward(meta_info)
        data.meta_info['format_and_function_call_reward'] = format_and_function_call_reward
        data.meta_info['finish_reward'] = finish_reward

        for i in range(batch_size):
            for j in range(len(each_turn_end_loc[i]) - 1):
                reward_tensor[i, each_turn_end_loc[i][j]] = format_and_function_call_reward[i][j]
            reward_tensor[i, each_turn_end_loc[i][-1]] = finish_reward[i] + pass_rewards[i]
            
            if i < self.num_examine:
                print(f"\n[Reward Sample {i}]")
                print(f"  Response: {all_trajectories[i][:200]}...")
                print(f"  Pass reward: {pass_rewards[i]}")
                print(f"  Format reward: {format_and_function_call_reward[i]}")
                print(f"  Finish reward: {finish_reward[i]}")
        
        return reward_tensor
    
    def _extract_query(self, full_prompt: str) -> str:
        import re
        # ä¿®æ”¹ï¼šä¸¥æ ¼æå–<|im_start|>user\nå’Œ<|im_end|>ä¹‹é—´çš„å†…å®¹
        pattern = r'<\|im_start\|>user\n(.*?)<\|im_end\|>'
        matches = re.findall(pattern, full_prompt, re.DOTALL)
        if matches:
            # å¦‚æœæœ‰å¤šä¸ªï¼Œå–æœ€åä¸€ä¸ª
            query = matches[-1].strip()
            return query
        return full_prompt.strip()

    def _compute_format_and_function_call_reward(self, meta_info: Dict) -> List[List[float]]:
        turns_stats = meta_info['turns_stats']
        valid_action_stats = meta_info['valid_action_stats']
        valid_api_call_stats = meta_info['valid_api_call_stats']
        batch_size = len(turns_stats)
        format_rewards = [[] for _ in range(batch_size)]
        
        for i in range(batch_size):
            for j in range(turns_stats[i] - 1):
                if valid_action_stats[i][j] and valid_api_call_stats[i][j]:
                    format_rewards[i].append(0.1)
                elif valid_action_stats[i][j] and not valid_api_call_stats[i][j]:
                    format_rewards[i].append(-0.1)
                else:
                    format_rewards[i].append(-0.2)
        
        return format_rewards

    def _compute_finish_reward(self, meta_info: Dict) -> List[float]:
        """
        Compute finish reward
        Args:
            sample_idx: sample index
            meta_info: meta information
        Returns:
            finish reward for each sample
        """
        finish_called = meta_info['finish_called']
        finish_rewards = []
        for i in range(len(finish_called)):
            if finish_called[i]:
                finish_rewards.append(0.2)
            else:
                finish_rewards.append(-0.5)
        return finish_rewards

    def _get_remote_pass_rewards(self, queries: List[str], trajectories: List[str]) -> List[float]:
        """é€šè¿‡ HTTP è°ƒç”¨è¿œç¨‹ Reward Server"""
        payload = {
            "queries": queries,
            "trajectories": trajectories
        }
        try:
            response = requests.post(self.reward_server_url, json=payload, timeout=60)
            if response.status_code == 200:
                return response.json().get("scores", [0.5] * len(queries))
            else:
                print(f"Remote server error: {response.status_code}")
                return [0.0] * len(queries)
        except Exception as e:
            print(f"Failed to connect to reward server: {e}")
            return [0.0] * len(queries)



def create_toolbench_reward_manager(
    tokenizer,
    **kwargs
) -> ToolBenchRewardManager:
    return ToolBenchRewardManager(
        tokenizer=tokenizer,
        **kwargs
    )
